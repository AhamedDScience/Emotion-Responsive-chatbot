{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9af69ab-b894-4553-9f31-553d8149147b",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598e0251-56e9-4dfe-924f-1ab5be77dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import threading\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image and video processing\n",
    "import cv2\n",
    "from PIL import Image, ImageTk, ImageDraw, ImageFont\n",
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# Audio processing\n",
    "import pyaudio\n",
    "import wave\n",
    "import pygame\n",
    "\n",
    "# Machine learning and deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Computer vision\n",
    "import face_recognition\n",
    "\n",
    "# Speech recognition and synthesis\n",
    "import speech_recognition as sr\n",
    "from pyht import Client, TTSOptions, Format\n",
    "from elevenlabs import Voice, VoiceSettings\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "# GUI\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog\n",
    "\n",
    "# AI and language models\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54842e1a-95f4-4019-bf97-0f24bbb87fec",
   "metadata": {},
   "source": [
    "## Configuration for Audio Recording, Text-to-Speech, and Google API Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f4e30f-d6d6-4c2d-ae21-84e179d21a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording parameters\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "AUDIO_OUTPUT = \"output.wav\"\n",
    "VIDEO_OUTPUT = \"output.avi\"\n",
    "FINAL_OUTPUT = \"final_output.mp4\"\n",
    "\n",
    "# Global variables\n",
    "recording = False\n",
    "audio_frames = []\n",
    "cap = None\n",
    "out = None\n",
    "selected_microphone = None\n",
    "\n",
    "# TTS Client\n",
    "client = Client(\"User ID\", \"API Code\")\n",
    "\n",
    "# Google API Key setup\n",
    "os.environ[\"GOOGLE_API_KEY\"] = 'GOOGLE_API_KEY'\n",
    "gemini_model  = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\", temperature=0.6)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['history', 'input'],\n",
    "    template='''You are a nice friend and imagine you have the ability to remember the past conversation and the ability to see the face of the conversation's human. Use the past conversation if it's needed. Based on the conversation chat emotion and the human face emotion, alter the responses and rank the human chat response within these emotions: sad, fearful, angry, love, embarrassed, happy, neutral.\n",
    "Use the following format:\n",
    "Response: response here in a beautiful manner\n",
    "Rank the emotion of the human chat input: only mention the one ranked emotion value here nothing more needed here\n",
    ".\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94703d86-f396-47b9-8a6a-7e61a7895966",
   "metadata": {},
   "source": [
    "## Load emotion detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8c89b9-4269-4f16-8650-8fda7fdc38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion detection model\n",
    "with open('emotion_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "emotion_model.load_weights(\"emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5451a-0b43-4742-987d-cd16e6173add",
   "metadata": {},
   "source": [
    "## Function to Find Face Encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59f611f-8e04-4a09-85b6-a183b5839b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_encodings(file):\n",
    "    img = cv2.imread(file)\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    encode = face_recognition.face_encodings(rgb_img)[0]\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2832a-c776-4f66-980d-41df0678eaa0",
   "metadata": {},
   "source": [
    "## Script for Face Encoding, Chat History Management, Audio Processing, and Emotion Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9c6a6b-a829-4e50-8502-0c5a93aceb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding face encodings for specific images\n",
    "enc_face_img2 = find_encodings('chris.jpg')\n",
    "\n",
    "# Sample chat history\n",
    "chat1 = '''Elon Musk is a rich person.'''\n",
    "\n",
    "# Creating a DataFrame as a database\n",
    "df = pd.DataFrame(columns=['encode', 'chat_history'])\n",
    "new_row2 = {\"encode\": enc_face_img2, \"chat_history\": chat1}\n",
    "df.loc[len(df)] = new_row2\n",
    "\n",
    "# Function to separate audio from video and extract text\n",
    "def separate_audio(video_file, audio_path):\n",
    "    video = VideoFileClip(video_file)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_path)\n",
    "    \n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    return text\n",
    "\n",
    "# Setting important variables\n",
    "emotion_dict = {\n",
    "    0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", \n",
    "    3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"\n",
    "}\n",
    "\n",
    "audio_path = \"output_audio.wav\"  # Temporary file path to save the audio\n",
    "\n",
    "# Regular expression patterns\n",
    "response_pattern = r'Response: (.*) Rank the emotion'\n",
    "emotion_pattern = r'Rank the emotion of the human chat input: (\\w+)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97366b-ffc5-4b0a-865a-7bffd7ce6515",
   "metadata": {},
   "source": [
    "# Function to separate audio from video and extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5995cede-282e-46d6-943d-f2f1b5ad36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transcribing(file_path):\n",
    "    # Function to separate audio from video and extract text\n",
    "    video = VideoFileClip(file_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_path)\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52098f59-ee60-40fe-b5c8-818450595d3b",
   "metadata": {},
   "source": [
    "# Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c66eda-6502-4587-81cb-2e38ca1e6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_match(db,frame):\n",
    "    match = []\n",
    "    # Face recognition\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "    \n",
    "    for face_encoding in face_encodings:\n",
    "        face_matches = [face_recognition.compare_faces([face_encode], face_encoding,tolerance = 0.5) for face_encode in db.encode]\n",
    "        match.extend(face_matches)\n",
    "    return match,face_encodings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda094d4-f418-4dec-80d2-3a1d369e8204",
   "metadata": {},
   "source": [
    "# Emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8354c963-3c9a-4e41-85a5-834bf4afe02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_detection(frame):            \n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    labels = []\n",
    "    # Emotion detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        max_index = int(np.argmax(emotion_prediction))\n",
    "        labels.append(emotion_dict[max_index])\n",
    "        print(labels)\n",
    "    most_common_value = Counter(labels).most_common(1)[0][0]\n",
    "    print(f\"Most common emotion: {most_common_value}\")\n",
    "    return most_common_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05cc80-1836-4145-a388-390e8751a7aa",
   "metadata": {},
   "source": [
    "# Function to handle chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085ea8df-7e1c-4613-a431-ec86eccada50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_history(db,match,face_encodings,text,most_common_value):\n",
    "    index = None\n",
    "    chat_history = False\n",
    "    if match:\n",
    "        flat_list = [item for sublist in match for item in sublist]\n",
    "        try:\n",
    "            index = flat_list.index(True)\n",
    "            chat_history = True\n",
    "            print(\"Match found\")\n",
    "        except ValueError:\n",
    "            print(\"No match found in existing data\")\n",
    "    \n",
    "    if not chat_history:\n",
    "        print(\"No history, creating new entry\")\n",
    "        new_row = {\"encode\": face_encodings[0], \"chat_history\": ''}\n",
    "        db.loc[len(db)] = new_row\n",
    "        index = len(db) - 1\n",
    "    \n",
    "    combined_input = f\"human face emotion: {most_common_value} question: {text}\"\n",
    "    chat = str(db.iloc[index, 1]) if index is not None else \"\"\n",
    "    return combined_input,chat,index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeff6db-40c8-497a-bc8b-2959c2fb4d5e",
   "metadata": {},
   "source": [
    "# Function to handle LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5590f70a-bb0b-4527-a4c5-531e09e0b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_function(chat,combined_input,db,index):\n",
    "    conversation_with_summary = ConversationChain(\n",
    "        llm=gemini_model ,\n",
    "        prompt=prompt_template,\n",
    "        memory=ConversationSummaryBufferMemory(\n",
    "            llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.6),\n",
    "            max_token_limit=30,\n",
    "            moving_summary_buffer=chat\n",
    "            )\n",
    "        )\n",
    "    print(\"The LLM is running\")\n",
    "    # Running the chain to generate output\n",
    "    output = conversation_with_summary.predict(input=combined_input)\n",
    "    print(output)    \n",
    "\n",
    "    memory = conversation_with_summary.memory.moving_summary_buffer\n",
    "    db.iloc[index, 1] = memory\n",
    "    print('Summary updated successfully')\n",
    "    print(f\"Updated summary: {db.iloc[index, 1]}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f724a-4c1f-4eba-b632-ad1e23128d40",
   "metadata": {},
   "source": [
    "# Extracting response and emotion from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9babbb07-6994-449e-a64a-9fa546954ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_extract(output):\n",
    "    # Extracting response and emotion from the output\n",
    "    response_pattern = r'Response: (.+?)(?:\\n|$)'\n",
    "    emotion_pattern = r'Rank the emotion of the human chat input: (\\w+)'\n",
    "    \n",
    "    response_match = re.search(response_pattern, output)\n",
    "    response = response_match.group(1).strip() if response_match else None\n",
    "    \n",
    "    emotion_match = re.search(emotion_pattern, output)\n",
    "    emotion_word = emotion_match.group(1) if emotion_match else 'NO Emotion'\n",
    "    \n",
    "    final_emotion = [emotion_word]\n",
    "    return response, final_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f7a1a-71f5-4711-b9db-2e65e1cfc9a0",
   "metadata": {},
   "source": [
    "# Function to handle TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15a0430-d76f-4525-9900-ea9d8f20ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_tts(response, final_emotion):\n",
    "    # Initialize pygame mixer\n",
    "    pygame.mixer.init()\n",
    "\n",
    "    emotions = ['sad', 'fearful', 'angry']\n",
    "    final_emotion = [x.lower() for x in final_emotion]\n",
    "    print(f\"Processed input: {final_emotion}\")\n",
    "\n",
    "    base_options = {\n",
    "        \"voice\": \"s3://voice-cloning-zero-shot/d9ff78ba-d016-47f6-b0ef-dd630f59414e/female-cs/manifest.json\",\n",
    "        \"sample_rate\": 8000,\n",
    "        \"format\": Format.FORMAT_MP3,\n",
    "    }\n",
    "\n",
    "    if final_emotion[0] in emotions:\n",
    "        options = TTSOptions(**base_options, speed=0.7)\n",
    "    else:\n",
    "        options = TTSOptions(**base_options, speed=0.9)\n",
    "\n",
    "    # Create a bytes buffer to store the audio data\n",
    "    audio_buffer = io.BytesIO()\n",
    "\n",
    "    for chunk in client.tts(text=[response], voice_engine=\"PlayHT2.0-turbo\", options=options):\n",
    "        audio_buffer.write(chunk)\n",
    "\n",
    "    # Reset the buffer position to the beginning\n",
    "    audio_buffer.seek(0)\n",
    "\n",
    "    # Load the audio data into pygame\n",
    "    pygame.mixer.music.load(audio_buffer)\n",
    "\n",
    "    # Play the audio\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Wait for the audio to finish playing\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "    # Clean up\n",
    "    pygame.mixer.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7f3c4-0b2d-4d57-a701-2817f01def60",
   "metadata": {},
   "source": [
    "# The main process code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f05e1760-5deb-45cf-b2d4-dec315bfe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jessica():\n",
    "    # Assuming these are global variables or imported from elsewhere\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Video files\", \"*.mp4;*.avi\")])\n",
    "    if not file_path:\n",
    "        return\n",
    "\n",
    "    def process_video():\n",
    "        update_status(\"Transcribing video...\")\n",
    "        text = Transcribing(file_path)\n",
    "        \n",
    "        update_status(\"Detecting emotion and performing face recognition...\")\n",
    "        face_data = analyze_video(file_path)\n",
    "        \n",
    "        update_status(\"Generating response...\")\n",
    "        response, final_emotion = generate_response(face_data, text)\n",
    "        \n",
    "        update_status(\"Playing response...\")\n",
    "        play_tts(response, final_emotion)\n",
    "        \n",
    "        update_status(\"Process completed!\")\n",
    "\n",
    "    threading.Thread(target=process_video).start()\n",
    "\n",
    "def analyze_video(file_path):\n",
    "    match = \"\"\n",
    "    face_encodings = []\n",
    "    most_common_emotion = \"\"\n",
    "    \n",
    "    vid = cv2.VideoCapture(file_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 20 == 0:\n",
    "            match, face_encodings = face_match(db,frame)\n",
    "            current_emotion = emotion_detection(frame)\n",
    "            if current_emotion:\n",
    "                most_common_emotion = current_emotion  # Simplified; consider using a proper method to determine the most common emotion\n",
    "                print(most_common_emotion)\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return match, face_encodings, most_common_emotion\n",
    "\n",
    "def generate_response(face_data, text):\n",
    "    try:\n",
    "        match, face_encodings, most_common_emotion = face_data\n",
    "        combined_input, chat,index = chat_history(db, match, face_encodings, text, most_common_emotion)\n",
    "        output = llm_function(chat, combined_input, db, index)\n",
    "        response, final_emotion = response_extract(output)\n",
    "        return response, final_emotion\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        return \"I'm sorry, there was an error processing the response.\", [\"neutral\"]\n",
    "\n",
    "\n",
    "\n",
    "db = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241d2a4-e999-40c5-a711-ac4506e1aed9",
   "metadata": {},
   "source": [
    "## Exit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff96ee6-fd10-4898-b274-594690eead26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_application():\n",
    "    # Release any resources that need to be closed\n",
    "    if 'cap' in globals() and cap is not None:\n",
    "        cap.release()\n",
    "    if 'out' in globals() and out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Stop any ongoing recordings\n",
    "    global recording\n",
    "    recording = False\n",
    "    \n",
    "    # Quit Pygame mixer if it's initialized\n",
    "    if pygame.mixer.get_init():\n",
    "        pygame.mixer.quit()\n",
    "    \n",
    "    # Destroy the window and exit the application\n",
    "    window.destroy()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec046b3-d388-4a53-af4d-ad3459442c56",
   "metadata": {},
   "source": [
    "## status_label Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c6732a-384e-4482-8944-758f81054fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(message):\n",
    "    status_label.config(text=message)\n",
    "    window.update_idletasks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018d09f-f864-4f27-b333-dd384b858b5d",
   "metadata": {},
   "source": [
    "## the upload function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff474d9-8cbf-4851-b0da-d7d2b6054212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the upload_video function\n",
    "def upload_video():\n",
    "    jessica()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b0640-fe8e-4c4f-aa27-06e3cfaf9fb3",
   "metadata": {},
   "source": [
    "## Setup GUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e95355-4b67-485b-8d6a-4b2bd176e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "['Surprised']\n",
      "Most common emotion: Surprised\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "['Sad']\n",
      "Most common emotion: Sad\n",
      "Sad\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "['Neutral']\n",
      "Most common emotion: Neutral\n",
      "Neutral\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "['Fearful']\n",
      "Most common emotion: Fearful\n",
      "Fearful\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "['Surprised']\n",
      "Most common emotion: Surprised\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['Neutral']\n",
      "Most common emotion: Neutral\n",
      "Neutral\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "['Neutral']\n",
      "Most common emotion: Neutral\n",
      "Neutral\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "['Surprised']\n",
      "Most common emotion: Surprised\n",
      "Surprised\n",
      "No match found in existing data\n",
      "No history, creating new entry\n",
      "The LLM is running\n",
      "Response: It's lovely to meet you, Ahmed! ðŸ˜Š \n",
      "Rank the emotion of the human chat input: neutral. \n",
      "\n",
      "Summary updated successfully\n",
      "Updated summary: Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: human face emotion: Surprised question: I am Ahmed\n",
      "\n",
      "New summary:\n",
      "A human, looking surprised, introduces themself as Ahmed. \n",
      "\n",
      "Processed input: ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# Setup GUI\n",
    "window = tk.Tk()\n",
    "window.title(\"Jessica\")\n",
    "window.attributes('-fullscreen', True)\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width = window.winfo_screenwidth()\n",
    "screen_height = window.winfo_screenheight()\n",
    "\n",
    "# Create and configure styles\n",
    "style = ttk.Style()\n",
    "style.theme_use('clam')\n",
    "\n",
    "style.configure('Modern.TButton', \n",
    "                font=('Helvetica', 12, 'bold'),\n",
    "                foreground='white',\n",
    "                background='#FF4547',\n",
    "                padding=(20, 10),\n",
    "                borderwidth=0,\n",
    "                relief='flat')\n",
    "style.map('Modern.TButton',\n",
    "          foreground=[('active', 'white')],\n",
    "          background=[('active', '#D70003')])\n",
    "\n",
    "style.configure('TLabel', font=('Helvetica', 12), background='#1E1E1E', foreground='white')\n",
    "style.configure('TFrame', background='#1E1E1E')\n",
    "style.configure('TCombobox', \n",
    "                selectbackground='#3E3E3E',\n",
    "                fieldbackground='#3E3E3E',\n",
    "                background='#3E3E3E',\n",
    "                foreground='white',\n",
    "                arrowcolor='white')\n",
    "style.map('TCombobox', fieldbackground=[('readonly', '#3E3E3E')])\n",
    "\n",
    "style.configure('Exit.TButton', \n",
    "                font=('Helvetica', 12, 'bold'),\n",
    "                foreground='white',\n",
    "                background='#FE8645',\n",
    "                padding=(20, 10),\n",
    "                borderwidth=0,\n",
    "                relief='flat')\n",
    "style.map('Exit.TButton',\n",
    "          foreground=[('active', 'white')],\n",
    "          background=[('active', '#B23F00')])\n",
    "\n",
    "# Create overlay\n",
    "overlay = ttk.Frame(window, style='TFrame')\n",
    "overlay.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Set background image\n",
    "bg_image = Image.open(\"jes2.jpg\")\n",
    "bg_image = bg_image.resize((screen_width, screen_height), Image.LANCZOS)\n",
    "bg_photo = ImageTk.PhotoImage(bg_image)\n",
    "bg_label = tk.Label(overlay, image=bg_photo)\n",
    "bg_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Create frames and widgets\n",
    "input_frame = ttk.Frame(overlay, style='TFrame')\n",
    "input_frame.place(relx=0.98, rely=0.02, anchor='ne')\n",
    "\n",
    "status_label = ttk.Label(overlay, text=\"\", style='TLabel', font=('Helvetica', 14))\n",
    "status_label.place(relx=0.5, rely=0.9, anchor='center')\n",
    "\n",
    "button_frame = ttk.Frame(overlay, style='TFrame')\n",
    "button_frame.place(relx=0.98, rely=0.98, anchor='se')\n",
    "\n",
    "upload_button = ttk.Button(button_frame, text=\"Upload Video\", command=upload_video, style='Modern.TButton')\n",
    "upload_button.pack(side=tk.TOP, padx=5, pady=5)\n",
    "\n",
    "exit_button = ttk.Button(overlay, text=\"Exit\", command=exit_application, style='Exit.TButton')\n",
    "exit_button.place(relx=0.02, rely=0.02, anchor='nw')\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd8a52-1818-41d2-aa8e-9efebf238e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fd92b-2da6-424e-a5de-474d156f45b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
